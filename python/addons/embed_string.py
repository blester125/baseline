"""As of 2/04/2018 the flair library on PyPI disables external loggers (turning
off baseline reporting log). They fix this in the next version. Please install
with:

git clone https://www.github.com/zalandoresearch/flair.git
cd flair
pip install .

This will install the newest version which fixes this.
"""

import os
import shelve
import torch
import torch.nn as nn
from flair.data import Sentence
from flair.embeddings import FlairEmbeddings, StackedEmbeddings
from baseline.train import (
    ConstantScheduler,
    register_lr_scheduler,
    register_training_func,
    create_trainer,
)
from baseline.model import register_model
from baseline.utils import get_model_file, get_metric_cmp, listify
from baseline.embeddings import register_embeddings
from baseline.pytorch.embeddings import PyTorchEmbeddings
from baseline.pytorch.tagger.model import RNNTaggerModel


class Cache(object):
    """A cache that can be either in memory or use shelve to store it on disk.

    This cache is designed to handle numpy arrays so the serialization methods
    only work on them.

    The cache strives to be fast while the user doesn't have to know if it is
    in memory or on disk so it exposes a serialize method that transforms the
    key into the format used by the cache, this avoids the need to have to
    serialize multiple times for cache checking, inserting, and getting.

    This is meant to be a write once read many times cache so there is no
    eviction policy as that would provide almost no value due to how we train.

    Currently the cache cannot be used across runs which is odd. I am looking
    into this.
    """
    def __init__(self, file_name=None):
        self.file_name = file_name
        self.cache = shelve.open(file_name) if file_name is not None else {}
        self.serialize = Cache.to_string if file_name is not None else Cache.to_bytes

    @staticmethod
    def to_string(key):
        return key.tobytes().decode('latin-1')

    @staticmethod
    def to_bytes(key):
        return key.tobytes()

    def __getitem__(self, key):
        return self.cache[key]

    def __contains__(self, key):
        return key in self.cache

    def __setitem__(self, key, value):
        self.cache[key] = value

    def __del__(self):
        if self.file_name is not None:
            self.cache.close()
            # Currently the cache isn't usable across runs so we will
            # delete it to stop many runs from blowing up your disk
            for ext in ('.bak', '.dat', '.dir'):
                try:
                    os.remove(self.file_name + ext)
                except:
                    pass


@register_embeddings(name='flair-string')
class ContextualStringEmbeddings(nn.Module, PyTorchEmbeddings):
    """Use Pretrained Flair Embeddings.

    Note:
        Expects raw text from the `dict_text` vectorizer.

    :param name: `str` The name of the embedding

    Keyword Arguemnts:
        - finetune `bool` Should we finetune these pre-trained embeddings. Must
            be False to cache
        - no_cache `bool` Should we cache the representations generated by these
            embeddings. Used to turn off caching even if not fine tuning.
        - disk_cache `str` The name of the on disk cache to use. Defaults to
            "flair_cache", set it to null to use in-memory cache (a dict).
            On my laptop in-memory cache causes stuttering and freezing of
            other things so I would recommend the disk cache.
        - device `str` What device should the embeddings be computed on?
            Defaults to 'cuda'.
    """
    def __init__(self, name, **kwargs):
        super(ContextualStringEmbeddings, self).__init__()
        # These are based on the pretrained model so we don't really use them
        self.vsz = None
        self.dsz = 100
        self.finetune = kwargs.get('finetune', False)
        fwd = FlairEmbeddings('news-forward')
        bwd = FlairEmbeddings('news-backward')
        self.embed = StackedEmbeddings(embeddings=[fwd, bwd])
        self.no_cache = bool(kwargs.get('no_cache', False))
        self.cache = Cache(kwargs.get('disk_cache', 'flair_cache'))
        # We only get to look at the embedding input for ourselves (strings)
        # which are always on cpu so we expose a device option to specify
        # That we want calculations done on the cpu.
        self.device = kwargs.get('device', 'cuda')

    def encode(self, x):
        if self.finetune or self.no_cache:
            return self.run(x)
        # The flair LM is quite slow so we will cache results when not finetuneing.
        # Caching times (in memory cache):
        #  train: 259.348 -> 32.397
        #  valid: 109.146 ->  5.831
        # Caching times (on disk cache):
        #  train: 252.604 -> 35.129
        #  valid: 123.347 ->  9.676
        cacheable_x = self.cache.serialize(x)
        if cacheable_x not in self.cache:
            embed = self.run(x).detach()
            self.cache[cacheable_x] = embed.cpu()
        return self.cache[cacheable_x].to(self.device)

    def run(self, x):
        # Flair doesn't publish a vocab or a tokenization scheme so we join
        # our text on space and just let their library handle it :(
        sentences = [Sentence(' '.join(s)) for s in x]
        bsz, mxlen = x.shape
        embedding = torch.zeros((mxlen, bsz, self.embed.embedding_length)).to(self.device)
        self.embed.embed(sentences)
        # Re-stack the embeddings into a dense, regularly shaped tensor.
        for i, sentence in enumerate(sentences):
            for j, token in enumerate(sentence):
                embedding[j, i, :] = token.embedding
        return embedding

    def get_dsz(self):
        return self.embed.embedding_length

    def get_vsz(self):
        return self.vsz


@register_model(task='tagger', name='flair-string')
class SkippingKeyTagger(RNNTaggerModel):
    """torch.from_numpy doesn't work on a string array so we need to skip it."""

    def input_tensor(self, key, batch_dict, perm_idx):
        if key == 'string':
            return batch_dict[key]
        return super(SkippingKeyTagger, self).input_tensor(key, batch_dict, perm_idx)


class DecayScheduler(ConstantScheduler):
    """The Flair paper trains with a schedule where when the dev set performance
    is stagnant for some window they reduce the learning rate."""
    def __init__(self, window=5, reduction=2, **kwargs):
        super(DecayScheduler, self).__init__(**kwargs)
        self.window = window
        self.reduction = reduction

    def decay(self, offset):
        if offset > self.window:
            self.lr /= self.reduction


@register_lr_scheduler(name='decay')
class DecaySchedulerPyTorch(DecayScheduler): pass


@register_training_func('tagger', 'decay')
def fit(model, ts, vs, es, **kwargs):
    """This fit function gives the learning rate scheduler access to the
    number of epochs since dev last improved."""

    do_early_stopping = bool(kwargs.get('do_early_stopping', True))
    epochs = int(kwargs.get('epochs', 20))
    model_file = get_model_file('tagger', 'pytorch', kwargs.get('basedir'))
    conll_output = kwargs.get('conll_output', None)
    txts = kwargs.get('txts', None)

    best_metric = 0
    if do_early_stopping:
        early_stopping_metric = kwargs.get('early_stopping_metric', 'acc')
        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get('early_stopping_cmp'))
        patience = kwargs.get('patience', epochs)
        print('Doing early stopping on [%s] with patience [%d]' % (early_stopping_metric, patience))

    reporting_fns = listify(kwargs.get('reporting', []))
    print('reporting', reporting_fns)

    after_train_fn = kwargs.get('after_train_fn', None)
    trainer = create_trainer(model, **kwargs)

    last_improved = 0
    for epoch in range(epochs):

        # Decay LR if you haven't improved recently
        offset = epoch - last_improved
        trainer.optimizer.lr_function.decay(offset)
        trainer.train(ts, reporting_fns)
        if after_train_fn is not None:
            after_train_fn(model)
        test_metrics = trainer.test(vs, reporting_fns, phase='Valid')

        if do_early_stopping is False:
            model.save(model_file)

        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):
            last_improved = epoch
            best_metric = test_metrics[early_stopping_metric]
            print('New best %.3f' % best_metric)
            model.save(model_file)


        elif (epoch - last_improved) > patience:
            print('Stopping due to persistent failures to improve')
            break

    if do_early_stopping is True:
        print('Best performance on %s: %.3f at epoch %d' % (early_stopping_metric, best_metric, last_improved))

    if es is not None:
        print('Reloading best checkpoint')
        model = torch.load(model_file)
        trainer = create_trainer(model, **kwargs)
        trainer.test(es, reporting_fns, conll_output=conll_output, txts=txts, phase='Test')
